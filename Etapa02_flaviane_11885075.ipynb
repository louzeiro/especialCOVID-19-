{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font size=\"6\">   Universidade de São Paulo -  USP/São Carlos\n",
    "    \n",
    "    Instituto de Ciências Matemáticas e de Computação - ICMC\n",
    "    \n",
    "    Estatística e Ciência de Dados\n",
    "    \n",
    "    Grupo: Flaviane Louzeiro da Silva - nUSP 11885075\n",
    "    \n",
    "    SCC0652 - Visualização Computacional \n",
    "    \n",
    "    Projeto Prático\n",
    " \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivo Geral\n",
    "\n",
    "Neste trabalho prático pretende-se explorar a base de dados CORD-19,  afim de clusterizar os artigos. Para isso os termos mais frequentes, as nacionalidades mais citadas, a similaridade dos documentos com TF-IDF e por fim o agrupamento de documentos com o <i>k-means</i>. Dessa forma, este trabalho é divido em, pelo menos, 03 etapas.\n",
    "\n",
    "<b> Etapa 01 - Organizando o dataframe </b>\n",
    "\n",
    "Inicialmente será apresentada a base de dados e a estrutura original, em seguida é feito o tratamento da base de dados, realizando a validação da estrutura dos artigos analisados, organizando-os em um <i>dataframe</i>, retirando os artigos com atributos faltosos e os artigos duplicados.\n",
    "\n",
    "<b> Etapa 02 - Exploração do dataframe e calculo de similaridade dos documentos </b>\n",
    "\n",
    "Afim de completar a etapa anterior, será realizada a tokinização dos termos, em seguida a sumarização dos dados tratados, com os termos mais frequentes e a nuvem de palavras. Em seguinda será realizada a busca pelas entidades nomeadas, utilizando recursos de processamento de linguagem natural.\n",
    "\n",
    "<b> Etapa 03 - Agrupamento dos documentos</b>\n",
    "\n",
    "[Em desenvolvimento...] \n",
    "\n",
    "\n",
    "Abaixo segue os links para cada etapa.\n",
    "\n",
    "[Etapa 01](#etapa1)\n",
    "\n",
    "[Etapa 02](#etapa2)\n",
    "\n",
    "[Etapa 03](#etapa3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 01 <a id='etapa1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descrição da base de dados <a id='descricao'></a>\n",
    "\n",
    "Com o objetivo de ajudar pesquisadores e corpo médico no combate ao Covid-19, foi lançado um desafio Kaggle[1] para tratamento e busca de informações em uma base de dados formada por diversos artigos científicos relacionados ao corona vírus, o **COVID-19 Open Research Dataset (CORD-19)**.\n",
    "\n",
    "  Atualmente, o CORD-19 possue mais de 200.000 artigos, destes pelos menos 100.000 são artigos completos, gerando em torno de 20GB. No entanto, para esta tarefa usei uma base com apenas 4GB, que contém mais de 29.000 artigos, onde pelo menos 13.000 desses são artigos completos. Esses artigos estão formatados na estrutura *.JSON*\n",
    "\n",
    "[1] https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Etapa 02] (#etapa_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização da estrutura do dataset <a id='visualizacao'></a>\n",
    "  \n",
    "   O dataset CORD-19 possuem estrutura apresentada abaixo, com 4 diretórios principais. \n",
    "   <img src=\"figs/estruturaCORD-19.png\"/>\n",
    "   \n",
    "   Dentro de cada diretório há os artigos em formato *JSON*. \n",
    "   <img src=\"figs/arquivosJSON.png\" />\n",
    "   \n",
    "   Os artigo *.JSON* possuem estão estruturados no formato abaixo:\n",
    "   <img src=\"figs/estrutruraJSON.png\"/>\n",
    "   \n",
    "   Onde o título se apresenta conforme a figura a seguir\n",
    "   <img src=\"figs/title.png\"/>\n",
    "   \n",
    "   O resumo está contido na seguinte estrutura.\n",
    "   <img src=\"figs/abstract.png\"/>\n",
    "   \n",
    "   E por fim, o corpo do artigo encontra-se no formato abaixo, onde cada subseção *1,2...* contém um paragrafo da publicação.\n",
    "   <img src=\"figs/corpo.png\"/>\n",
    "   \n",
    "   \n",
    "   Esta exploração da estrutura do arquivo é necessária pois os códigos desenvolvidos terá que alimentar um dataframe com as informações contidas em cada seção dos artigos contidos no *CORD-19*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metodologia\n",
    "\n",
    "Conforme visto em aula, a organização e seleção dos dados é a primeiro etapa do pipeline de visualização. Dessa forma a persistência dos dados é crucial nesse processo. \n",
    "\n",
    "Em resumo, nesta etapa será realizada a:\n",
    "* inserção dos artigos em um dataframe;\n",
    "* exclusão dos artigos que por ventura estejam faltando alguma seção como ID, título, resumo, corpo do texto;\n",
    "* remoção de artigos duplicados\n",
    "\n",
    "A seguir será apresentada a metodologia utilizada para manipulação do dataset, onde o objetivo é organizar o dataset em um dataframe para utilizá-lo posteriormente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalação e carregamento das Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definição da estrutura do dataframe\n",
    "Inicialmente será criado um dicionário vazio, que será utilizado para buscar na base de dados os seguintes atributos nos artigos:\n",
    "* a identificação (paper_id);\n",
    "* o título (title)\n",
    "* o resumo (abstract)\n",
    "* o corpo (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicArtigos = {'paper_id':[], 'title':[],\n",
    "             'abstract':[], 'text':[]}\n",
    "type(dicArtigos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente o dicionário é transformado em dataframe, para facilitar a manipulação no decorrer do processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfArtigos = pd.DataFrame.from_dict(dicArtigos)\n",
    "type(dfArtigos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listagem dos artigos contidos no dataset\n",
    "Criada a estrutura, será feita uma busca recursiva pelos arquivos .json presentes no diretório com o auxílio da blibioteca de leitura de diretorio glob. Os arquivos *.json* serão lidos e o nome do arquivo será inserido na lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foram encontrados  29315  artigos\n"
     ]
    }
   ],
   "source": [
    "listaArtigos = glob.glob(f'{\"./\"}//**/*.json', recursive= True) \n",
    "\n",
    "print('Foram encontrados ',len(listaArtigos), ' artigos') # total de artigos percorrido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando o dataframe\n",
    "A seguir será criada a função que de fato alimentará o dataframe com os atributos definidos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funcao que vai percorrer todos os aquivos e popular o DF\n",
    "def funcao_dfArtigos(listaArtigos, df):\n",
    "    for nomeArtigo in listaArtigos:\n",
    "        linhaDF={'paper_id':None, 'title':None,\n",
    "                 'abstract':None, 'text':None}\n",
    "        \n",
    "        with open(nomeArtigo) as json_data:\n",
    "            \n",
    "            dados = json.load(json_data)\n",
    "            \n",
    "            linhaDF['paper_id']=dados['paper_id'].strip() #retirando os espaços no inicio e no fim\n",
    "            linhaDF['title']=dados['metadata']['title'] \n",
    "            # extração palavra por palavra do abstract\n",
    "            listaAbstract = [abstract['text'] for abstract in dados['abstract']]\n",
    "            abstract = '\\n'.join(listaAbstract)\n",
    "            linhaDF['abstract']=abstract.strip()\n",
    "            # extração palavra por palavra do conteudo\n",
    "            listaTexto = [text['text'] for text in dados['body_text']] \n",
    "            texto = '\\n'.join(listaTexto)\n",
    "            linhaDF['text']=texto.strip()\n",
    "            \n",
    "            df = df.append(linhaDF, ignore_index = True)\n",
    "        \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chamada da função. Agora de fato ocorrerá a extração dos dados e insersão no dataframe.\n",
    "\n",
    "**Observação: Este procedimento poderá demorar alguns minutos.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos = funcao_dfArtigos(listaArtigos, dfArtigos) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('O dataframe foi alimentado com ',dfArtigos.shape[0], ' artigos e ', dfArtigos.shape[1], ' atributos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## salvando o total de artigos para comparar\n",
    "totalArtigos=dfArtigos.shape[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificação da Base de dados\n",
    "\n",
    "Inicialmente será verificado no dataframe se há valores faltantes, ou seja, se no dataframe os artigos foram  alimentado com todos os atributos definidos no dicionário. \n",
    "\n",
    "Então, por se tratar de uma base de dados textual será será feito um mapa de calor com o auxilio da biblioteca *seaborn*. A intenção é visualmente saber em quais artigos há atributos faltantes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dfArtigos.isnull());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observação:** De acordo com o mapa de calor, em todos os artigos foram carregadas alguma informação em cada atributo, seja informação útil ou apenas um espaço. No entanto, conforme é possível notar no head do dataframe, há artigos sem título ou sem abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação dos atributos\n",
    "\n",
    "Dessa forma será verificado em todas as colunas do dataframe se há algum atributo com apenas um espaço, ou seja ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificando se há artigo sem ID\n",
    "print ('Foram encontrados ',len(dfArtigos[dfArtigos['paper_id']=='']), 'artigos sem o ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Foram encontrados ',len(dfArtigos[dfArtigos['title']=='']),'artigos sem o título')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Foram encontrados ',len(dfArtigos[dfArtigos['abstract']=='']), 'artigos sem o resumo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Foram encontrados ',len(dfArtigos[dfArtigos['text']=='']),'artigos sem conteúdo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remoção dos artigos  \n",
    "Feita as verificações, será excluído da base de dados os artigos com informações faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## apagando os artigos que faltam informações\n",
    "dfArtigos = dfArtigos[dfArtigos['title'] != ''] # deixando somente os artigos que tem titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos = dfArtigos[dfArtigos['abstract'] != ''] # deixando somente os artigos que tem abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Haviam ', totalArtigos,'após a limpeza ficaram, ', dfArtigos.shape[0],'artigos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo os artigos duplicados\n",
    "Por fim, usando a função do pandas *drop_duplicates*, será removido os artigos que estejam duplicados no dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remoção de artigos duplicados, o inplace = true remove e atualiza o df\n",
    "dfArtigos.drop_duplicates(['abstract','text','title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Haviam ', totalArtigos, 'artigos, após todas as verificações restaram ', dfArtigos.shape[0],'artigos')\n",
    "print('Uma redução de',round(100-dfArtigos.shape[0]*100/totalArtigos), '% da base de dados')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvando o dataframe\n",
    "\n",
    "Salvando o dataframe para o envio no Edisciplinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos.to_csv('dfArtigos_completo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "O tratamento da base de dados foi o primeiro passo dado. A base de dados é textual e, conforme apresentado anteriormente, houve uma redução de 31% após a verificação da base de dados. Com isso, verificou-se a importância da realização da persistência dos dados, visto que, embora o desafio siga regras e possivelmente tenha seguido uma política controle na inserção dos artigos no dataset, houveram artigos com seções faltosas além de artigos duplicados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 02 <a id='etapa2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo \n",
    "\n",
    "Identificar o grupo taxonômico dos dados, apresentar e avaliar o mapeamento visual apropriado. No entanto, para isso é necessário concluir a etapa anterior. Dessa forma, será dada continuidade no tratamento, realizando a tokinização e sumarização dos dados, apresentando-os  em uma nuvem de palavras e uma tabela com os termos mais frequentes. Para isso, além pré processamento já realizado, será necessário inicialmente tratar os dados com bibliotecas de processamento natural \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amostragem da base\n",
    "\n",
    "Embora já tenha ocorrido uma redução de 31% dos artigos após o pré processamento. Ainda há muitos artigos, o que resultou em um travamento de minha máquina. Dessa forma, daqui em diante, será trabalhado apenas com uma amostra do dataset, essa amostra foi retirada aleatoriamente da base de dados com 500 artigos. E, para que o processo possa ser repetido, será fixado o random_state = 123."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos_amostra = dfArtigos.sample(n=500, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos_amostra.shape # checando a estrutura do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos_amostra.head() # checando os 5 primeiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso da biblioteca científica\n",
    "\n",
    "Por se tratar de uma base de dados com artigos científicos, o inglês é a língua padrão e acredita-se que há uma baixa taxa de erros, visto que seguem regras bem explícitas pois são  publicados em revista.\n",
    "\n",
    "Será utilizada a biblioteca **spacy**, que é uma biblioteca para processamento avançado de linguagem natural, sendo bastante utilizada para analisar textos. Nela há o modelo **en_core_sci_md** já treinado na para o jargão médico. \n",
    "\n",
    "Dentre as funcionalidades do modelo há uma lista de stop words, que é uma lista de termos não representativos para um documento, geralmente essa lista é composta por: preposições, artigos, advérbios, números, pronomes e pontuação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### instalação das bibliotecas - Se necessário\n",
    "#pip install spacy\n",
    "#pip install scispacy\n",
    "#pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.5/en_core_sci_md-0.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import scispacy\n",
    "import en_core_sci_md #fazer download na cell acima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, será criado o modelo, onde serão desabilitados alguns parametros para facilitar o processamento. Além disso será definido o tamanho máximo de 2000000 caracteres a serem analisados em cada artigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = en_core_sci_md.load(disable=['tagger', 'parser', 'ner'])\n",
    "modelo.max_lenght = 2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração e limpeza dos termos nos Textos\n",
    "\n",
    "Para a maioria dos tipos de processamento lingüístico é necessário identificar e categorizar as palavras de um texto, para isso é realizada a extração e limpeza dos termos, onde cada documento vai ter o seu conteúdo dividido em cada palavra significantiva presente no documento.\n",
    "\n",
    "Primeiro  é realizada a tokenização para decompor o documento em cada termo que o compõe, delimitando o espaço em branco entre os termos, quebras de linhas, tabulações, e alguns caracteres especiais. \n",
    "\n",
    "Além disso, é feita a limpeza, onde são removidos as stop words, depois é apliacada a lematização  que é o método para redução de um termo ao seu radical, removendo as desinências, afixos, e vogais temáticas, dessa forma os termos derivados de um mesmo radical serão contabilizados como um único termo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokinizacao(sentenca):\n",
    "    sentenca = sentenca.lower() # colocando miusculo\n",
    "    \n",
    "    #lematização \n",
    "    lista = []\n",
    "    lista = [palavra.lemma_ for palavra in modelo(sentenca) if not (palavra.is_stop      #removendo as stop_words e\n",
    "                                                                   or palavra.like_num   #os numeros\n",
    "                                                                   or palavra.is_punct   # pontuacao\n",
    "                                                                   or palavra.is_space   # espaços excedentes \n",
    "                                                                   or len(palavra)== 1   # com tamanho 1 caractere                                                       \n",
    "                                                                   \n",
    "                                                                \n",
    "                                                                   )] \n",
    "    \n",
    "    lista = ' '.join([str(palavra) for palavra in lista]) #concatenando as palavras\n",
    "    \n",
    "    \n",
    "    return lista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testando a função no primeiro artigo da amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = dfArtigos_amostra['text'][10258] \n",
    "print('Antes da tokinizaçao: ', len(teste), ' palavras')\n",
    "print('Após a tokinização: ', len(tokinizacao(teste)),' palavras')\n",
    "print('Uma redução de ', round(len(tokinizacao(teste))/len(teste)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testada a função, agora será aplicada em todos os atributos 'text' dos artigos do dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos_amostra['text'] = dfArtigos_amostra['text'].apply(tokinizacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sumarização "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termos frequentes - Tabela de Frequencia\n",
    "Será usada a biblioteca NLTK, no entanto ela precisa que as entradas estejam em txt, assim, inicialmente será criado um txt para cada artigo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in dfArtigos_amostra.iterrows(): #percorendo as linhas do DF\n",
    "    artigoTXT = open('corpus/'+row['paper_id']+'.txt', 'w') # criando o .txt\n",
    "    n = artigoTXT.write(row['text'])# excrevendo no .txt\n",
    "    artigoTXT.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criando o Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus = PlaintextCorpusReader('corpus', '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = corpus.words()\n",
    "print('Há ',len(n), ' palavras no corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequenciaPalavra = nltk.FreqDist(n) # distribuição de frequencia\n",
    "maisFrequentes = frequenciaPalavra.most_common(100) # 100 mais comuns\n",
    "maisFrequentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Termos Frequentes - Nuvem de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coresMapa = ListedColormap(['orange', 'green', 'red','magenta']) # cores do mapa\n",
    "nuvemPalavras = WordCloud(background_color = 'white', max_words= 100, colormap = coresMapa)\n",
    "nuvemPalavras = nuvemPalavras.generate(dfArtigos_amostra['text'].str.cat(sep='\\n'))\n",
    "plt.figure(figsize = (15,15))\n",
    "plt.imshow(nuvemPalavras)\n",
    "plt.axis('off') # sem eixo x e y\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight - Extração de entidades nomeadas\n",
    "\n",
    "Utilizando recursos de processamento de linguagem natural, através da biblioteca spacy, será realizada a contagem dos países e nacionalidades mais citadas na amostra, fazendo busca pelas entidades nomeadas do tipo [GPE](https://spacy.io/api/annotation), ou seja, os paises, cidades e nacionalidades mais citadas na amostra. \n",
    "\n",
    "Para isso, será criando um modelo de reconhecimento de entidade na língua inglesa, com tamanho máximo de 2000000 caracteres.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_entidade = spacy.load('en')\n",
    "nlp_entidade.max_length = 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpe = []\n",
    "for index, row in dfArtigos_amostra.iterrows():\n",
    "    text = row['text']\n",
    "    doc = nlp_entidade(text)\n",
    "    for entidade in doc.ents:\n",
    "        if entidade.label_ == 'GPE':\n",
    "            gpe.append(str(entidade.text))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exibindo as entidades com mais de 50 citadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "gpe_valor, gpe_qtd = np.unique(np.array(gpe), return_counts = True)\n",
    "dfGPE = pd.DataFrame({'valor':gpe_valor, 'qtd': gpe_qtd})\n",
    "dfGPE_filtrado = dfGPE[dfGPE.qtd>50] #  mais de 50 citações\n",
    "dfGPE_filtrado = dfGPE_filtrado.sort_values(by='qtd', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barplot das 10 entidades nomeadas mais citadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(15,8)})\n",
    "sns.barplot(x= 'valor', y='qtd', hue='valor', data = dfGPE_filtrado[:10]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com seus dados pré-processados, identifique em qual grupo taxonômico seu conjunto de dados se encaixa (p.e. categórico, numérico, híbrido, etc...). Justifique com uma explicação dos seus dados.\n",
    "\n",
    "        Vide o catálogo From Data to Viz\n",
    "\n",
    "Identifique qual mapeamento visual é o mais indicado para os seus dados e, se você julgar que aquele paradigma visual realmente é a melhor escolha, apresente seus dados com a visualização\n",
    "\n",
    "        Caso decida utilizar outra visualização, justifique a escolha\n",
    "\n",
    "Descreva os insights que a visualização proporcionou para os seus dados\n",
    "\n",
    "Para as próximas etapas:\n",
    "\n",
    "    Definir modelo de representação dos documentos\n",
    "\n",
    "    Definir objetivos do que você deseja realizar no conjunto de dados\n",
    "\n",
    "Já existem vários sistemas que trabalham com exploração visual deste conjunto de dados: CORD-19. Pode usar estes como inspiração sobre o que explorar no conjunto de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos = pd.read_csv('corona_df_completo.csv')\n",
    "dfArtigos = dfArtigos.dropna()\n",
    "dfArtigos.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArtigos.to_csv('dfArtigos_completo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD-IDF  \n",
    "\n",
    "Para pontuar a importância de uma palavra em um documento com base na frequência com que ela apareceu nesse documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textosArtigos = dfArtigos['text'].tolist()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "tfdif = TfidfVectorizer(max_features=2**12) # para evitar estouro de memoria\n",
    "artigosVet = tfdif.fit_transform(textosArtigos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redução de dimensionalidade com PCA (<i>Principal Componet Analysis </i>) para vizualizar a dispersão dos artigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA (n_components=2) # 2 componentes\n",
    "x_pca = pca.fit_transform(artigosVet.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(10,8)})\n",
    "sns.scatterplot(x_pca[:,0],x_pca[:,1])\n",
    "plt.title('Artigos Covid');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etapa 03 <a id='etapa3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrupamento com k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
